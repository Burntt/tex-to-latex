% single - workload

RNNs only
\cite{wang2021algorithm}
\cite{bi2021integrated}
\cite{nguyen2019host}

Initially, Recurrent Neural Network (RNN) based models were he go-to choice, specifically designed to handle sequential data patterns . However, their performance often suffered due to two major drawbacks: a declining memory of previous data and sequential processing. 

% multi - workload
CNN
\cite{LSTNet} % justification
\cite{RPTCN}
However, traditional CNNs are generally considered to be
not suitable for modeling a long-time series problem now. This
is mainly due to the limitation of its convolution kernel, which
cannot fully capture the long-term dependence information
contained in the sequence. To solve this problem, a causal
CNN has been proposed.

CNN + RNN
\cite{xu2022esdnn}

To address the aforementioned challenges for cloud workload prediction, we first extract some
key features from the realistic traces derived from the cloud data center, and then convert the multivariate time series into supervised learning time series [14] for further training with our designed
training algorithm based on GRU. Our objective is to achieve efficient and accurate predictions for
highly variable and high dimensional cloud workloads to finally optimize the resource usage in
cloud computing environments.
The main contributions of this article are summarized as follows:
• The sliding window for Multivariate Time series Forecasting (S-MTF) is designed to
convert multivariate time series into supervised learning time series for multivariate workloads and keep sufficient information. The S-MTF can reorganize the time series to sample
X and label Y and model the correlation between predicted data, which can use algorithms
based on Deep Neural Network (DNN) to achieve predictions.
• An efficient supervised learning-based Deep Neural Network (esDNN) algorithm is proposed for cloud workload prediction to learn and capture the features of historical data and
accurately predict future workloads. The proposed algorithm can adapt to the variances of
workloads by updating the gates of GRU and overcome the limitations of gradient disappearance and explosion.
• Comprehensive experiments are conducted by using realistic data derived from Alibaba and
Google cloud data centers to evaluate the performance of esDNN. The results demonstrate
that the proposed approach can achieve better prediction accuracy than state-of-the-art algorithms. Experiments also show that the prop

RNN + (GNN / GAN / CNN)
\cite{li2024evogwp}
We design a two-level importance based shapelet extraction mechanism to mine new usage pattern changes in temporal dimension, and design a novel evolution graph model to fuse the interference among resource usage patterns of different workloads in spatial dimension. By combining temporal extraction of shapelets from each single workload and spatial interference of shapelets among different workloads, we then design a spatio-temporal GNN-based encoder-decoder model to predict the long-term dynamic changes of workloads. Experiments using real trace data from Alibaba, Tencent and Google show that EvoGWP improves the prediction accuracy by up to 58.6% over the state-of-the-art prediction methods. Moreover, EvoGWP can outperform the state-of-the-art prediction methods in terms of model convergence. To the best of our knowledge, this is the first work that explicitly identifies fine-grained workload resource usage patterns to accurately predic...

\cite{RNNGAN} 
n this paper, we address this issue by proposing a hybrid E2LG algorithm, which decomposes the cloud workload time-series into its constituent components in different frequency bands using empirical mode decomposition method which reduces the complexity and nonlinearity of prediction model in each frequency band. Also, a new state-of-the-art ensemble GAN/LSTM deep learning architecture is proposed to predict each sub band workload time-series individually, based on its degree of complexity and volatility. Our novel ensemble GAN/LSTM architecture, which employs stacked LSTM blocks as its generator and 1D ConvNets as discriminator, can exploit the long-term nonlinear dependencies of cloud workload time-series effectively specially in high-frequency, noise-like components. By validating our approach using extensive set of experiments with standard real cloud workload traces, we confirm that E2LG provides significant improvements in cloud workload prediction accuracy with respect to the mean absolute and standard deviation of the prediction error and outperforming traditional and state-of-the-art deep learning approaches. It improves the prediction accuracy at least 5% and 12% in average compared to the main contemporary approaches in recent papers such as hybrid methods which employs CNN, LSTM or SVR.
\cite{AGCRN}  % justification
 To
this end, we propose two adaptive modules for enhancing Graph Convolutional
Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning
(NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic
series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations
in traffic series automatically based on the two modules and recurrent networks.
Our experiments 1 on two real-world traffic datasets show AGCRN outperforms
state-of-the-art by a significant margin without pre-defined graphs about spatial
connections.

%% limitation
Graph Neural Networks (GNNs) have been introduced to capture spatial dependencies in data, and often, a combination of GNN with RNN or CNN is used to leverage both spatial and temporal information. Despite these advancements, these hybrid models still face limitations from their sub-components \cite{benidis2022deep}.

QNN
\cite{singh2021quantum}
Etc.

 The rotation and reverse rotation effects of the Controlled-NOT (C-NOT) gate serve activation function at the hidden and output layers to adjust the qubit weights. In addition, a Self Balanced Adaptive Differential Evolution (SB-ADE) algorithm is developed to optimize qubit network weights. The accuracy of the EQNN prediction model is extensively evaluated and compared with seven state-of-the-art methods using eight real world benchmark datasets of three different categories. Experimental results reveal that the use of the quantum approach to evolutionary neural network substantially improves the prediction accuracy up to 91.6 percent over the existing approaches.


