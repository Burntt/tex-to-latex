\section{OMNIFORE: FRAMEWORK FOR OPTIMIZATION OF RESOURCE FORECASTS IN EDGE-CLOUD NETWORKS}
\label{sec: Proposed Solution}

OmniFORE brings together several important innovations: a strategic method for sampling data, advanced techniques for clustering time-series data, an in-depth introduction to various attention mechanisms, hyperparameter optimization, and scaling strategies to handle large numbers of traces.

\subsection{Trace Sampling Strategy}
\label{sec: Trace Sampling Strategy}


\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{img/proposed_solution_trace_selection.pdf}
\caption{Trace sampling process for generalization training. Historical workload data are processed to extract container traces, which are clustered into representative groups.}
\label{fig:proposed_solution_trace_selection}
\end{figure}

To achieve robust generalization across diverse workloads, OmniFORE employs a novel approach to extract a representative set of historical traces from the edge-cloud network. This strategy ensures that the model trained on these traces can effectively generalize to various workload types. The detailed steps ($S_1$ - $S_5$) of this process are outlined in Fig. ~\ref{fig:proposed_solution_trace_selection}.

The goal of these steps is to extract a \textbf{set of historical traces}, denoted as \(\mathbf{T}_{\text{train}}(t)\), where \(N_{\text{train}}\) represents the number of training traces. This set is a representative subset of the entire set of traces, \(\mathbf{T}_{\text{all}}(t)\), which contains \(N_{\text{all}}\) traces. "Representative" means that \(\mathbf{T}_{\text{train}}(t)\) captures the essential characteristics and variations present in \(\mathbf{T}_{\text{all}}(t)\), ensuring that the training set accurately reflects the diversity and distribution of the full dataset:

\begin{equation}
\mathbf{T}_{\text{train}}(t) \in \mathbb{R}^{d_f \times d_t \times N_{\text{train}}} \subset \mathbf{T}_{\text{all}}(t) \in \mathbb{R}^{d_f \times d_t \times N_{\text{all}}},
\end{equation}
where $N_{\text{train}} < N_{\text{all}}$. Using all traces for training would be computationally prohibitive and costly.

Section \ref{sec: Container Trace Heterogeneity} underscores the diverse workloads handled by containers, which require efficient resource management through OmniFORE. The highly dynamic nature of container data, marked by their time-dependent spatio-temporal patterns, presents significant challenges for analysis. Traditional clustering algorithms often struggle with such complex time-series data \cite{gupta2020approaches}. To overcome this, we propose transforming the data into a latent space to create a dense representation, facilitating more efficient clustering and differentiation \cite{frazier2018tutorial}. This latent space captures essential features, enabling better management of the data's complexities.

First ($S_1$), OmniFORE extracts spatio-temporal features from the container trace time-space, denoted as $\mathcal{T}$. Each trace $T_i(t) \in \mathbb{R}^{d_f \times d_{\text{t}}}$ is then transformed into \textbf{a lower-dimensional latent space}, $\mathcal{Z} \in \mathbb{R}^{z}$, where $z \ll d_f \times d_t$. This transformation, defined as $\mathcal{Z}(z) = f_{\text{encoder}}(\mathcal{T}(t))$, is optimized to capture the essential spatio-temporal patterns of the data over time. Details are provided in Section \ref{sec: Unsupervised Temporal Clustering}. By converting the time-dependent space $\mathcal{T}$ into the latent space $\mathcal{Z}$, OmniFORE optimizes for the most informative features for clustering, enhancing both efficiency and effectiveness by reducing dimensionality while preserving essential temporal information.

Second ($S_2$), the latent space $\mathcal{Z}$ is then \textbf{clustered}. Each cluster $C_k$ groups all container traces with similar spatio-temporal patterns over time into $K$ groups. The clustering process can be represented as
\begin{equation}   
\{C_1, C_2, \ldots, C_K\} = \text{Cluster}(\mathcal{Z}(z)).
\end{equation}
Third ($S_3$), each cluster \(C_k\) labels the original time-series traces, preserving the unique characteristics of each trace for accurate model training while leveraging the efficiency of latent space clustering. This can be mathematically depicted as
\begin{equation}
\mathbf{T}_{\text{all, cluster}}(t) = \{T_{1,1}(t), T_{2,1}(t), \ldots, T_{N_\text{all},k}(t)\},
\end{equation}
where $T_{1,1}(t)$ denotes the first trace in cluster 1.

Fourth ($S_4$), \textbf{representative traces} from each cluster $C_k$ are selected for training the prediction model. The selection process is optimized to ensure that the training data is diverse and representative of the broader dataset. Given a total number of traces $N_{\text{all}}$ and a desired subset of $N_{\text{train}}$ traces, the selection process is proportional to the number of traces in each cluster.

Let $N_{C_k}$ denote the number of traces in cluster $C_k$. The proportion of traces in each cluster compared to the total number of traces is
\begin{equation}
p_k = \frac{N_{C_k}}{N_{\text{all}}} \in \left[ 0, 1 \right].
\end{equation}
The number of traces selected from each cluster $C_k$ for the training set is
\begin{equation}
N_{\text{train},k} = \left\lfloor p_k \cdot N_{\text{train}} \right\rfloor,
\end{equation}
where $N_{\text{train},k}$ is the number of traces selected from cluster $C_k$ for training.

Therefore, the total number of traces in the training set $T_{\text{train}}$ is the sum of the selected traces from each cluster, such that

\begin{equation}  
N_{\text{train}} = \sum_{k=1}^{K} N_{\text{train},k}.
\end{equation}

%% data split
Finally ($S_5$), the traces in the sampled training set are split into train/validation/test segments along the time dimension ($d_t = d_{\text{train}} + d_{\text{val}} + d_{\text{test}}$), resulting in:

\begin{equation}
\begin{aligned}
&\text{\textit{training set:}} \quad  & \mathbf{T}_{\text{train}} &\in \mathbb{R}^{d_{\text{train}} \times d_{\text{f}} \times N_{\text{train}}}, \\
&\text{\textit{validation set:}} \quad & \mathbf{T}_{\text{val}} &\in \mathbb{R}^{d_{\text{val}} \times d_{\text{f}} \times N_{\text{train}}}, \\
&\text{\textit{test set:}} \quad & \mathbf{T}_{\text{test}} &\in \mathbb{R}^{d_{\text{test}} \times d_{\text{f}} \times N_{\text{train}}}.
\end{aligned} 
\end{equation}

OmniFORE ensures that data is effectively utilized for training, validating, and testing the prediction models, resulting in robust and generalizable performance.

\subsection{Unsupervised Temporal Clustering}
\label{sec: Unsupervised Temporal Clustering}

Figure \ref{fig:latent_space_creation} provides an overview of the \textbf{latent space extractor}. The input time-series $\mathcal{T}(t)$ is transformed into a latent space $\mathcal{Z}(z)$ by the encoder function $f_{\text{encoder}}$. The decoder function $f_{\text{decoder}}$ then reconstructs $\mathcal{Z}(z)$ into the output time-series $\mathcal{R}(t)$, which approximates $\mathcal{T}(t)$. This neural network structure compresses the data into a latent space, captures its critical features, and then reconstructs the original data. Successful reconstruction with the latent space acting as a \textbf{bottleneck} indicates that the latent space effectively encapsulates the essential information of the input signal.

To utilize this dense representation, we modify a \textbf{supervised} (using correct cluster labels for each trace) CNN-based approach for time-series clustering from \cite{clusteringDL} to an \textbf{unsupervised} (without cluster labels) version within OmniFORE. This process involves feature extraction through convolutional and pooling operations on raw data, followed by clustering in the latent space.

% Fig. \ref{fig:2d_latent_space} shows a 2D Principal Component Analysis \cite{abdi2010principal} of clusters in the latent space, where each point represents a time-series trace, color-coded by cluster.

\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{img/latent_space_creation.png}
\caption{Overview of the latent space extractor. The input $\mathcal{T}(t)$ is encoded into latent space $\mathcal{Z}(z)$ by $f_{\text{encoder}}$ and then decoded by $f_{\text{decoder}}$ to reconstruct $\mathcal{R}(t) \approx \mathcal{T}(t)$.}
\label{fig:latent_space_creation}
\end{figure}

The architecture of the latent space extractor comprises the following layers:
\begin{enumerate}
    \item \textbf{Encoder}: Transforms an input trace $T_i$ into the latent space $\mathcal{Z}(z)$ using $\mathcal{Z}(z) = f_{\text{encoder}}(\mathcal{T}(t))$.
    \begin{itemize}
        \item \textbf{Input Layer}: \(d_{f,0} \times d_{t,0}\) neurons, where \(d_{t,0}\) is the series length and \(d_{f,0}\) is the number of features.
        \item Several repetitions \(r\) of combined convolutional and pooling layers, where \(d_{f,r}\) is the number of filters and \(d_{t,r}\) is the size of the sequence dimension after the \(r\)-th convolutional layer. The transformed sequence is \(T'(t') \in \mathbb{R}^{d_{f,r} \times d_{t,r} \times N_{\text{all}}}\).
        \begin{itemize}
            \item \textbf{Convolutional Layers ${\boldsymbol{C}}_{r}(t')$}: Parameters are convolution stride ($s_r$), filter size ($d_{f,r-1} \times l_r$), activation functions, trainable weights ($\boldsymbol{\omega}_r$), and biases ($\boldsymbol{b}_r$).
            \item \textbf{Pooling Layers $\boldsymbol{P}_r(t')$}: Downsampling strategy to reduce feature map size by averaging regions.
        \end{itemize}
        \item \textbf{Latent Space Layer}: Converts the flattened output of the last convolutional layers, $\boldsymbol{C}_r$, into a lower-dimensional latent space, $\mathcal{Z} \in \mathbb{R}^{z \times N_{\text{batch}}}$, where $z \ll d_{f,0} \times d_{t, 0}$.
    \end{itemize}
    \item \textbf{Decoder}: Converts $\mathcal{Z}(z)$ back to $\mathcal{T}(t)$ as $\mathcal{R}(t) = f_{\text{decoder}}(\mathcal{Z}(z)) \approx f_{\text{encoder}}^{-1}(\mathcal{Z}(z))$.
        \begin{itemize}
            \item \textbf{Multiple Transposed Convolution Layers}: Reconstruct the time-series input data from the latent representation, back to the original data shape. Transposed convolutions, also known as deconvolutions, upsample the input by reversing the operations of standard convolutional layers.
        \end{itemize}
\end{enumerate}
The training process can be described as:
\begin{enumerate}
    \item \textbf{Initialize}: Set up the Encoder-Decoder architecture, initialize weights and biases.
    \item \textbf{Load Input Data}: Prepare input traces as $\mathbf{T_{\text{all}}} \in \mathbb{R}^{ d_{t,0} \times d_{f,0} \times N_{\text{all}}}$, split into $N_{\text{batch}}$ batches. Here, $N_{\text{batch}}$ denotes the number of batches.
    \item \textbf{Forward Pass}: Compute the output of each layer for each batch.
    
    \begin{itemize}
        \item \textbf{Encoder Convolution Layer}:
        \begin{footnotesize}
        \begin{equation}
        \boldsymbol{C}_r(t') = \sum_{i=1}^{l_r} \sum_{j=1}^{d_{f, r-1}} T^{'}_R(i+s_r(t'-1), j) \boldsymbol{\omega}_r(i, j) + \boldsymbol{b}(r),
        \end{equation}
        \end{footnotesize}
        where $\boldsymbol{\omega}_r \in \mathbb{R}^{l_r \times d_f}$ and $\boldsymbol{b}(r)$ are the weights and bias of the $r$-th convolution filter.
        
        \item \textbf{Encoder Pooling Layer}:
        \begin{footnotesize}
        \begin{equation}
        \boldsymbol{P}_r(t') = \operatorname{MaxPool}\left(\boldsymbol{C}_r((t'-1)l_r+1), \ldots, \boldsymbol{C}_r(t' l_r)\right).
        \end{equation}
        \end{footnotesize}
        \item \textbf{Decoder}: Transposed convolutions for reconstruction.
    \end{itemize}
    \item \textbf{Loss Function}: Calculate the mean-square error across all batches between the reconstructed time-series and the actual input time-series as
    \begin{equation}
    \mathcal{L} = \frac{1}{2N_{\text{batch}}} \sum_{n=1}^{N_{\text{batch}}} \sum_{t=1}^{d_t} \Big(T_i(t) - R_i(t) \Big)^2.
    \end{equation}
    \item \textbf{Update}: Use gradient descent to update the weights and biases of the (transposed) convolutions ($\boldsymbol{\omega}_r$, $\boldsymbol{b}_r$) and optimize for minimizing the loss function.
    \item \textbf{Iterate}: Repeat with new batches until the model converges.
\end{enumerate}

This unsupervised approach compresses time-series data into a latent space, capturing essential features and enabling reconstruction. By adapting CNN-based clustering to work without labels, it achieves efficient feature extraction for selecting appropriate model training traces.

\subsection{Attention Mechanism for Extended Sequences}

The original transformer model \cite{vaswani2017attention} significantly enhances dependency capture, yet faces computational hurdles with long sequences \cite{wen2022transformers}. Recent developments in efficient transformers \cite{fasterandlightertransformers} and sparse attention techniques \cite{gorbett2023sparse, STRec, ma2024multivariate} mitigate these issues. Within OmniFORE, we utilize informers \cite{zhou2021informer}, which leverage sparse attention mechanisms to predict extensive time-series data efficiently. Informers are a viable alternative to traditional transformers, despite their implementation complexity, and a crucial element of our proposed solution.

\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{img/Transformer_Architecture.pdf}
\caption{High-level overview of the transformer architecture}
\label{fig:Transformer_Architecture}
\end{figure}

Figure \ref{fig:Transformer_Architecture} outlines the vanilla transformer architecture, comprising an encoder and decoder. The input sequence $T[t-d_x:t]$ is embedded into a higher-dimensional space $d_{\text{embed}}$. The encoder, with $N_e$ layers (repetitions of the complete block), applies self-attention ($H$) and feed-forward networks ($d_{ff}$) to capture dependencies. The decoder, with $N_d$ layers, generates predictions $\hat{T}[t+1:t+d_y]$ by attending to both encoded inputs and previous outputs. In the following sections, we provide a high-level understanding of this architecture, which covers both transformers and the informers.

\subsubsection*{Time-series Embedding}

The \texttt{DataEmbedding}, denoted as $\mathbf{E}$, combines three embeddings to form a robust representation of time-series input sequence. The \texttt{Value Embedding} \cite{yue2022ts2vec} maps raw input values to a higher-dimensional space via a convolutional layer, capturing intricate data patterns of the input sequence. The \texttt{Positional Embedding} \cite{kazemnejad2024impact} encodes the sequence order using sinusoidal functions, preserving structural information crucial for time-series analysis. The \texttt{Temporal Embedding} \cite{liu2023spatio} incorporates time-related features (e.g., hour, day) to provide context on temporal dependencies and seasonal trends. These components are summed to produce the final embedding matrix $\mathbf{E}$, as described in the equation:

\begin{equation}
\mathbf{E} = \mathbf{E}_{\text{value}} + \mathbf{E}_{\text{position}} + \mathbf{E}_{\text{temporal}} \in \mathbb{R}^{d_x \times d_{\text{embed}}}.
\end{equation}

This matrix serves as the input to the subsequent layers of the model, encapsulating both the intrinsic data patterns and the temporal structure of the time-series.

\subsubsection*{Introduction to Attention Mechanism}

\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{img/attention_goal.pdf}
\caption{Attention mechanism refining time-series embeddings $\mathbf{E}_i$ to $\mathbf{E}_i'$, with arrow thickness indicating the strength of influence.}
\label{fig:attention_goal}
\end{figure}

Starting with the initial embeddings $\mathbf{E} \in \mathbb{R}^{d_x \times d_{\text{embed}}}$, the attention mechanism (denoted as $\mathcal{A}$) refines these embeddings into a new set, $\mathbf{E}'$ \cite{vaswani2017attention}. This mechanism enables the model to selectively focus on specific time steps, analogous to human attention. For instance, when predicting future workload at a peak, both the model and a human analyst would focus on the pattern surrounding the current peak and compare it to similar peaks in the past. The attention weights determine the degree to which each original embedding $\mathbf{E}i \in \mathbb{R}^{d{\text{embed}}}$ contributes to its corresponding refined embedding $\mathbf{E}_i'$. As illustrated in Fig. \ref{fig:attention_goal}, thicker lines indicate stronger influence. The refined embeddings $\mathbf{E}'$ effectively combine underlying data patterns with temporal relationships, enhancing the model's capacity to capture complex dependencies across the sequence \cite{dai2019transformer}.

To construct the refined embeddings $\mathbf{E}_i'$, the attention mechanism employs three key components: queries, keys, and values \cite{vaswani2017attention}, as illustrated in Fig. \ref{fig:attention_workings}. The model transforms each embedding $\mathbf{E}_i$ into three vectors:

\begin{equation}
    \begin{aligned}
        \text{Query vector:} \quad \mathbf{Q}_i &= \mathbf{E}_i \mathbf{W_Q}, \\
        \text{Key vector:} \quad \mathbf{K}_i &= \mathbf{E}_i \mathbf{W_K}, \\
        \text{Value vector:} \quad \mathbf{V}_i &= \mathbf{E}_i \mathbf{W_V},
    \end{aligned}
\end{equation}
where $\mathbf{W_Q}, \mathbf{W_K}, \mathbf{W_V} \in \mathbb{R}^{d_{\text{embed}} \times d_{\text{embed}}}$ are learned weight matrices. Conceptually, a query vector encodes the information need at the current time step, key vectors facilitate the identification of relevant past information, and value vectors contain the actual content to be aggregated \cite{bahdanau2014neural}. In the context of workload prediction, when analyzing a peak, the query vector would encode features of the current peak, key vectors would help identify similar historical peaks, and value vectors would contain the associated historical workload patterns.

The model evaluates temporal relationships by computing scaled dot products between query and key vectors:

\begin{equation}
    \text{Score}(\mathbf{Q}_i, \mathbf{K}_j) = \frac{\mathbf{Q}_i \mathbf{K}_j^T}{\sqrt{d_{\text{embed}}}}.
\end{equation}

A high score indicates that the corresponding time step contains critical information for the current prediction, as the vectors align in high-dimensional space \cite{luong2015effective}. The scaling factor $\sqrt{d_{\text{embed}}}$ ensures training stability by preventing excessively large values that could lead to sharp, unbalanced distributions post-softmax \cite{xu2015show}. The softmax function normalizes these scores into a probability distribution, effectively assigning weights to each timestep's embedding vector. These weights, summing to one, determine the relative importance of each timestep, guiding the attention mechanism to focus on the most relevant parts of the sequence.

To capture diverse temporal dependencies, the model employs \textit{multi-headed attention} \cite{michel2019sixteen} with $H$ parallel attention mechanisms, or "heads" (Fig. \ref{fig:attention_workings}). Each head $h \in \{1, ..., H\}$ operates independently on the same input embeddings, but with its own set of learnable weight matrices $\mathbf{W}_Q^h, \mathbf{W}_K^h, \mathbf{W}_V^h \in \mathbb{R}^{d_{\text{embed}} \times d_{\text{head}}}$, where $d_{\text{head}} = d_{\text{embed}} / H$. These matrices are initialized differently, leading to distinct evolutionary trajectories during training and convergence to different local optima~\cite{zhang2019improving}. This architectural design enables the model to simultaneously project the input into multiple subspaces and attend to various aspects of the sequence, such as short-term fluctuations and long-term trends, effectively increasing the model's representational capacity without a proportional increase in computational complexity \cite{vaswani2017attention}.

For each head $h$, the attention output is computed as

\begin{equation}
    \mathbf{A}_i^h = \mathcal{A}(\mathbf{E}_i\mathbf{W}_Q^h, \mathbf{E}_i\mathbf{W}_K^h, \mathbf{E}_i\mathbf{W}_V^h).
\end{equation}

The outputs from all heads are concatenated and linearly transformed to produce the multi-headed attention output

\begin{equation}
    \mathbf{MHA}_i = d_{ff}(\operatorname{Concat}(\mathbf{A}_i^{(1)}, \mathbf{A}_i^{(2)}, \ldots, \mathbf{A}_i^{(H)})),
\end{equation}
where $d_{ff}: \mathbb{R}^{d_{\text{embed}} \times H} \rightarrow \mathbb{R}^{d_{\text{embed}}}$ represents a linear transformation layer. This layer projects the concatenated attention outputs from all $H$ heads, with total dimension $d_{\text{embed}} \times H$, back to the original embedding dimension $d_{\text{embed}}$.

The final refined embedding for each time step incorporates a residual connection \cite{he2016deep} and layer normalization \cite{lei2016layer}:

\begin{equation}
    \mathbf{E}_i' = \text{LayerNorm}(\mathbf{E}_i + \mathbf{MHA}_i),
\end{equation}
where LayerNorm denotes layer normalization. This is followed by a position-wise feed-forward network ($d_{ff}$) with another residual connection and layer normalization:

\begin{equation}
    \mathbf{E}_i'' = \text{LayerNorm}(\mathbf{E}_i' + d_{ff}(\mathbf{E}_i')).
\end{equation}

This entire process—multi-headed attention, residual connections, layer normalizations, and feed-forward network—constitutes one layer of the transformer encoder. It is repeated across multiple stacked layers ($N_e$). Each layer operates on the output of the previous one, allowing for iterative refinement of embeddings and progressively developing intricate representations crucial for accurate predictions in complex time-series tasks.

\begin{figure}
\centering
\includegraphics[width=0.42\textwidth]{img/attention_workings.pdf}
\caption{Multi-Headed Attention: Input embeddings ($\mathbf{E}_i$) are transformed into query ($\mathbf{Q}$), key ($\mathbf{K}$), and value ($\mathbf{V}$) vectors. Multiple heads ($H$) capture different temporal aspects. Larger circles indicate stronger attention scores.}
\label{fig:attention_workings}
\end{figure}

\subsubsection*{Traditional Attention Mechanism}

The original self-attention mechanism \cite{vaswani2017attention} is defined by

\begin{equation}
\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \operatorname{Softmax}\left(\frac{\mathbf{QK}^\top}{\sqrt{d_{\text{embed}}}}\right) \mathbf{V},
\end{equation}
where $\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{d_x \times d_{\text{embed}}}$ are the query, key, and value matrices, respectively, while $d_{\text{embed}}$ is the embedding dimension of each point in the input sequence of length $d_x$. The product $\mathbf{QK}^\top \in \mathbb{R}^{d_x \times d_x}$ measures the importance of relationships between every data point in the sequence. This attention mechanism has $\mathcal{O}(d_x^2)$ complexity, making it inefficient for long input sequences.


\subsubsection*{Efficient Attention Mechanism}

The three architectural innovations of informers compared to the original transformer can be summarized as follows:
\subsubsection*{$\text{Informer}^{+1}$: Computationally Efficient Attention}

The sparse self-attention mechanism enhances efficiency by concentrating on the most critical queries. This is done by forming a sparse matrix $\overline{\mathbf{Q}}$ that retains only the most influential queries from $\mathbf{Q}$, with other entries set to zero. The selection of these important queries is directed by a sparsity measurement \cite{zhou2021informer}. This method significantly reduces computational complexity, making it more suitable for long sequences, as it scales linearly with the natural logarithm of the sequence length: $\mathcal{O}(d_x \log d_x)$.

\subsubsection*{$\text{Informer}^{+2}$: Memory Saving with Attention Distilling}

Due to the presence of zeros in the input matrix $\overline{\mathbf{Q}}$, the attention matrix $\mathcal{A}$ contains unnecessary entries. Removing these zero entries conserves memory and enables the model to concentrate on significant entries, a process known as attention distilling.

In each attention layer, the encoder utilizes self-attention distilling to minimize memory consumption by emphasizing the dominant entries in the attention matrix, thereby generating a new matrix

\begin{equation}
\mathcal{A}_{\text{distil}} = \operatorname{MaxPool}\left(\operatorname{ELU}\left(\operatorname{Conv1d}\left(\mathcal{A}\right)\right)\right),
\end{equation}
where $\operatorname{Conv1d}$ applies 1-D convolutional filters along the time dimension, and the activation function $\operatorname{ELU}$~\cite{al2023fhic} is used. The result is then passed through a max-pooling layer to downsample $\mathcal{A}$. This process decreases the time dimension of the input, reducing memory usage to $\mathcal{O}(d_x \log d_x)$.

\subsubsection*{$\text{Informer}^{+3}$: Fast Inference}

The decoder accelerates predictions for long sequences by utilizing generative inference, which forecasts the entire sequence in one step instead of predicting each time step sequentially. This is accomplished by using a known "start token" in conjunction with placeholders for the future sequence, formulated as

\begin{equation}
T_{\mathrm{de}}^{t} = \operatorname{Concat}\left(T_{\text{token}}^{t}, T_{\mathbf{0}}^{t}\right) \in \mathbb{R}^{(d_{\text{token}} + d_y) \times d_{\text{embed}}},
\end{equation}
where $T_{\text{token}}^{t} \in \mathbb{R}^{d_{\text{token}} \times d_{\text{embed}}}$ is the start token, and $T_{\mathbf{0}}^{t} \in \mathbb{R}^{d_y \times d_{\text{embed}}}$ acts as a placeholder for the future sequence. This generative inference approach significantly reduces computation time while preserving accuracy.



\subsection{Scaling}

Container traces in edge-cloud computing exhibit significant variability in resource usage, necessitating individual normalization to ensure balanced training. OmniFORE normalizes each trace such that the mean of the metric under study is zero (i.e., $\mu_i=0$) and the variance is one (i.e., $\sigma_i^2=1$), thereby preventing traces with higher means and variances from overshadowing others and enhancing model generalization.

Given a training set $\mathbf{T_{\text{train}}} \in \mathbb{R}^{d_t \times d_{\text{f}} \times N_{\text{train}}}$, each trace $T_i(t)$ is normalized as follows:

\begin{equation}
\tilde{T}_i(t) = \frac{T_i(t) - \mu_i}{\sigma_i},  
\end{equation}
where $\tilde{T}_i(t)$ denotes the normalized version of the trace $T_i(t)$, \(\mu_i\) is the mean, and \(\sigma_i = \sqrt{\sigma_i^2}\) is the standard deviation of the \(i\)-th trace. The mean \(\mu_i\) and variance \(\sigma_i^2\) are calculated as

\begin{equation}
\mu_i = \frac{1}{d_t} \sum_{t=1}^{d_t} T_i(t), \quad \sigma_i^2 = \frac{1}{d_t} \sum_{t=1}^{d_t} (T_i(t) - \mu_i)^2.
\end{equation}

The scaling parameters $(\mu_i, \sigma_i^2)$ are saved for each trace. During testing, these parameters are used to invert the normalization of predictions back to the original scale, such that

\begin{equation}
\hat{T}_i(t) = \tilde{T}_i(t) \cdot \sigma_i + \mu_i,
\end{equation}
where $i$ indexes each trace to maintain the scaling parameters for each trace.

\subsection{Bayesian Optimization with Generalization Objective}
\label{sec:BayesianOptimizationWithGeneralizationObjective}

Optimizing hyperparameters is essential for improving model performance and generalization \cite{hyperparamsdatasets}. Due to the impracticality of traditional grid search in expansive hyperparameter spaces \cite{gridsearchvsbayesian}, Bayesian optimization \cite{frazier2018tutorial} offers an efficient alternative by constructing a probabilistic model of the objective function and iteratively selecting the most promising hyperparameters.

Figure \ref{fig:gp_toy_example} illustrates Bayesian optimization, starting with initial observations (red dots) of the objective function. This simplified example reduces the hyperparameter space to prediction length $d_y$, though in practice it is multidimensional (hyperparameter vector $\mathbf{h}$). A model (blue line) predicts the objective function's mean and confidence interval (CI, shaded area). The next hyperparameter (star) is selected based on predicted high performance and uncertainty, balancing exploration and exploitation.

OmniFORE's custom objective function $\text{Obj}(\mathbf{h})$ aims to maximize generalization across diverse container traces:
\begin{equation}
    \text{Obj}(\mathbf{h}) = \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \text{Perf}(\mathbf{h}, T_{\text{val}, i}),
\end{equation}
where $N_{\text{val}}$ is the number of validation traces, $T_{\text{val}, i}$ is the $i$-th validation trace, and $\text{Perf}(\mathbf{h}, T_{\text{val}, i})$ represents the performance metric.

\begin{figure}
\centering
\includegraphics[width=0.42\textwidth]{img/gp_toy_example.pdf}
\caption{Bayesian optimization toy example. The star indicates the next value to try ($d_y = 5$). The target signifies the goal of minimizing the objective function.}
\label{fig:gp_toy_example}
\end{figure}
